"""
Live Transcription + Diarization z u≈ºyciem Gradio
Transkrypcja przez Groq Whisper + rozpoznawanie m√≥wc√≥w przez whisper-diarization (CUDA)
"""

import os
import io
import re
import sys
import time
import glob
import queue
import shutil
import threading
import tempfile
import subprocess
from dataclasses import dataclass, field
from typing import List, Optional

# ≈Åadowanie .env.test je≈õli istnieje
try:
    from dotenv import load_dotenv
    if os.path.exists(".env.test"):
        load_dotenv(".env.test", override=True)
        print("‚úì Za≈Çadowano .env.test")
except ImportError:
    pass

import numpy as np
import gradio as gr
import requests

# Groq dla transkrypcji
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è Groq nie jest zainstalowany - zainstaluj: pip install groq")

# SoundDevice dla streamingu audio w czasie rzeczywistym
try:
    import sounddevice as sd
    SOUNDDEVICE_AVAILABLE = True
except ImportError:
    SOUNDDEVICE_AVAILABLE = False
    print("‚ö†Ô∏è sounddevice nie jest zainstalowany - streaming audio nie bƒôdzie dzia≈Ça≈Ç")

# ==================== KONFIG ====================
CHUNK_SECONDS = 10          # d≈Çugo≈õƒá kawa≈Çka dla transkrypcji
CHUNK_OVERLAP_SECONDS = 2.0  # overlap miƒôdzy chunkami (aby nie ucinaƒá zda≈Ñ) - 2 sekundy
TARGET_SR = 16000           # docelowa pr√≥bka WAV
MAX_RENDER_LINES = 120

# Pobierz konfiguracjƒô z env vars
def get_config(key: str, default: Optional[str] = None) -> Optional[str]:
    """Pobiera warto≈õƒá z env vars"""
    return os.getenv(key, default)

GROQ_API_KEY = get_config("GROQ_API_KEY")
DEEPGRAM_API_KEY = get_config("DEEPGRAM_API_KEY")
WHISPER_DIARIZATION_DIR = get_config("WHISPER_DIARIZATION_DIR", "./whisper-diarization") or "./whisper-diarization"
FORCED_LANG = get_config("LANG", "pl") or "pl"  # Polski domy≈õlnie
DEVICE_CONFIG = get_config("DEVICE", "cuda") or "cuda"  # CUDA domy≈õlnie
WHISPER_MODEL = get_config("WHISPER_MODEL", "small") or "small"  # Domy≈õlnie small (dla GTX 1050 Ti), mo≈ºna zmieniƒá na medium/large-v3

# Wyb√≥r silnika diarization
USE_DEEPGRAM = get_config("USE_DEEPGRAM", "true") or "true"
USE_DEEPGRAM = USE_DEEPGRAM.lower() == "true"

# Sprawd≈∫ klucze API
if USE_DEEPGRAM:
    if not DEEPGRAM_API_KEY:
        raise ValueError("‚ö†Ô∏è DEEPGRAM_API_KEY nie jest ustawiony! Sprawd≈∫ .env.test")
    print("[Config] ‚úÖ U≈ºywam Deepgram API do diarization")
else:
    if not GROQ_API_KEY:
        raise ValueError("‚ö†Ô∏è GROQ_API_KEY nie jest ustawiony! Sprawd≈∫ .env.test")
    print("[Config] ‚úÖ U≈ºywam whisper-diarization (lokalne)")

# Sprawd≈∫ czy CUDA jest dostƒôpne
def check_cuda_available():
    """Sprawdza czy CUDA jest dostƒôpne i dzia≈Ça"""
    if DEVICE_CONFIG.lower() != "cuda":
        return False, "DEVICE nie jest ustawiony na 'cuda'"
    
    # Sprawd≈∫ torch
    try:
        import torch
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            cuda_version = torch.version.cuda
            driver_version = torch.cuda.get_driver_version()
            print(f"[Config] ‚úì PyTorch widzi CUDA:")
            print(f"    UrzƒÖdzenie: {device_name}")
            print(f"    CUDA Runtime: {cuda_version}")
            print(f"    Sterownik CUDA: {driver_version}")
            
            # Spr√≥buj utworzyƒá tensor na CUDA (sprawdzi czy sterownik dzia≈Ça)
            try:
                x = torch.tensor([1.0]).cuda()
                del x
                torch.cuda.empty_cache()
                print(f"[Config] ‚úì Test CUDA zako≈Ñczony pomy≈õlnie")
                return True, "PyTorch CUDA dzia≈Ça"
            except RuntimeError as e:
                error_msg = str(e)
                if "driver version is insufficient" in error_msg.lower():
                    print(f"[Config] ‚ùå Sterownik CUDA jest za stary!")
                    print(f"    Wymagana wersja: {cuda_version}")
                    print(f"    Zainstalowana wersja: {driver_version}")
                    return False, f"Sterownik CUDA za stary (wymagane: {cuda_version}, masz: {driver_version})"
                else:
                    print(f"[Config] ‚ö†Ô∏è B≈ÇƒÖd testu CUDA: {e}")
                    return False, f"B≈ÇƒÖd testu CUDA: {e}"
        else:
            return False, "PyTorch zainstalowany, ale CUDA nie jest dostƒôpne"
    except ImportError:
        print("[Config] ‚ö†Ô∏è PyTorch nie jest zainstalowany - sprawdzam ctranslate2...")
    except Exception as e:
        print(f"[Config] ‚ö†Ô∏è B≈ÇƒÖd sprawdzania PyTorch CUDA: {e}")
    
    # Sprawd≈∫ ctranslate2
    try:
        import ctranslate2
        devices = ctranslate2.get_supported_compute_types("cuda")
        if len(devices) > 0:
            print(f"[Config] ‚úì ctranslate2 widzi CUDA: {devices}")
            return True, "ctranslate2 CUDA dzia≈Ça"
        else:
            return False, "ctranslate2 zainstalowany, ale CUDA nie jest dostƒôpne"
    except ImportError:
        print("[Config] ‚ö†Ô∏è ctranslate2 nie jest zainstalowany")
    except Exception as e:
        print(f"[Config] ‚ö†Ô∏è B≈ÇƒÖd sprawdzania ctranslate2 CUDA: {e}")
    
    return False, "Brak zainstalowanych bibliotek CUDA (torch lub ctranslate2)"

# Opcja wymuszenia CUDA (pomi≈Ñ sprawdzanie)
FORCE_CUDA_STR = get_config("FORCE_CUDA", "false") or "false"
FORCE_CUDA = FORCE_CUDA_STR.lower() == "true"

# Globalna flaga - czy CUDA nie dzia≈Ça (aby nie pr√≥bowaƒá za ka≈ºdym razem)
CUDA_FAILED = False

# Automatyczny fallback na CPU je≈õli CUDA nie dzia≈Ça
DEVICE = DEVICE_CONFIG
if DEVICE_CONFIG.lower() == "cuda":
    if FORCE_CUDA:
        print("[Config] ‚ö†Ô∏è FORCE_CUDA=true - u≈ºywam CUDA bez sprawdzania")
        DEVICE = "cuda"
    else:
        cuda_available, reason = check_cuda_available()
        if not cuda_available:
            print(f"[Config] ‚ö†Ô∏è CUDA nie jest dostƒôpne: {reason}")
            print("[Config] üí° RozwiƒÖzania:")
            print("   1. Zainstaluj PyTorch z CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            print("   2. LUB zainstaluj ctranslate2 z CUDA")
            print("   3. LUB ustaw FORCE_CUDA=true w .env.test aby wymusiƒá CUDA (mo≈ºe nie dzia≈Çaƒá)")
            print("   4. LUB ustaw DEVICE=cpu w .env.test")
            print("[Config] ‚ö†Ô∏è Prze≈ÇƒÖczam na CPU...")
            DEVICE = "cpu"
        else:
            print("[Config] ‚úì CUDA jest dostƒôpne - u≈ºywam GPU")
elif DEVICE_CONFIG.lower() == "cpu":
    print("[Config] ‚úì U≈ºywam CPU (mo≈ºe byƒá wolniejsze ni≈º CUDA, ale zawsze dzia≈Ça)")
else:
    print(f"[Config] ‚ö†Ô∏è Nieznane urzƒÖdzenie '{DEVICE_CONFIG}' - u≈ºywam CPU")
    DEVICE = "cpu"

# Klienci API
if GROQ_AVAILABLE:
    groq_client = Groq(api_key=GROQ_API_KEY)
else:
    groq_client = None

# ==================== POMOCNICZE ====================
def to_wav_bytes(pcm_float32: np.ndarray, sample_rate: int = TARGET_SR) -> bytes:
    import soundfile as sf
    # mono
    if pcm_float32.ndim > 1:
        pcm_float32 = pcm_float32.mean(axis=0)
    pcm = np.clip(pcm_float32, -1.0, 1.0)
    pcm16 = (pcm * 32767.0).astype(np.int16)
    buf = io.BytesIO()
    sf.write(buf, pcm16, sample_rate, format="WAV", subtype="PCM_16")
    return buf.getvalue()

def parse_srt(srt_path: str):
    """
    Parsuje prosty plik SRT zwracajƒÖc listƒô: [(start_sec, end_sec, speaker_label, text), ...]
    """
    out = []
    time_pat = re.compile(r"(\d\d):(\d\d):(\d\d),(\d\d\d)\s*-->\s*(\d\d):(\d\d):(\d\d),(\d\d\d)")
    speaker_pat = re.compile(r"^(?:Speaker\s*(\d+)|SPEAKER[_\s]?(\d+))\s*:\s*(.*)$", re.IGNORECASE)
    with open(srt_path, "r", encoding="utf-8", errors="ignore") as f:
        block = []
        for line in f:
            line = line.rstrip("\n")
            if line.strip() == "":
                if block and len(block) >= 2:
                    m = time_pat.search(block[1])
                    if m:
                        h1,m1,s1,ms1,h2,m2,s2,ms2 = map(int, m.groups())
                        start = h1*3600 + m1*60 + s1 + ms1/1000.0
                        end   = h2*3600 + m2*60 + s2 + ms2/1000.0
                        text_lines = block[2:] if len(block) > 2 else []
                        text = " ".join(t.strip() for t in text_lines if t.strip())
                        spk = None
                        msp = speaker_pat.match(text)
                        if msp:
                            spk = msp.group(1) or msp.group(2)
                            text = msp.group(3).strip()
                            spk = f"Speaker {spk}"
                        out.append((start, end, spk, text))
                block = []
            else:
                block.append(line)
        # ostatni blok
        if block and len(block) >= 2:
            m = time_pat.search(block[1])
            if m:
                h1,m1,s1,ms1,h2,m2,s2,ms2 = map(int, m.groups())
                start = h1*3600 + m1*60 + s1 + ms1/1000.0
                end   = h2*3600 + m2*60 + s2 + ms2/1000.0
                text_lines = block[2:] if len(block) > 2 else []
                text = " ".join(t.strip() for t in text_lines if t.strip())
                spk = None
                msp = speaker_pat.match(text)
                if msp:
                    spk = msp.group(1) or msp.group(2)
                    text = msp.group(3).strip()
                    spk = f"Speaker {spk}"
                out.append((start, end, spk, text))
    return out

@dataclass
class Segment:
    start: float
    end: float
    text: str
    speaker: Optional[str] = None

@dataclass
class ConversationState:
    started_at: float = field(default_factory=time.time)
    transcript: List[Segment] = field(default_factory=list)

# Globalny stan rozmowy
conversation_state = ConversationState()

# Bezpieczne przechowywanie danych do UI
_ui_data_lock = threading.Lock()
_ui_data = {
    "transcript_text": "",
    "speakers_text": "_Oczekiwanie na transkrypcjƒô..._",
    "chunks_created": 0,
    "chunks_processed": 0,
    "processing_status": "‚è∏Ô∏è Oczekiwanie na chunki...",
    "processing_progress": 0.0,
    "current_chunk": 0,
    "total_chunks": 0,
}

# Kolejka audio
audio_q: "queue.Queue[tuple[bytes, float]]" = queue.Queue()

# Globalna flaga - czy model zosta≈Ç wczytany
MODEL_PRELOADED = False
MODEL_PRELOAD_LOCK = threading.Lock()

# ==================== PRELOAD MODEL ====================
def preload_whisper_model(device: str, compute_type: str, model_name: str) -> bool:
    """
    Wstƒôpnie wczytuje model Whisper do pamiƒôci GPU/CPU z progress barem.
    Zwraca True je≈õli sukces, False je≈õli b≈ÇƒÖd.
    """
    global MODEL_PRELOADED
    
    with MODEL_PRELOAD_LOCK:
        if MODEL_PRELOADED:
            print("[Preload] ‚úÖ Model ju≈º zosta≈Ç wczytany wcze≈õniej")
            return True
        
        print("\n" + "=" * 80)
        print("[Preload] üöÄ ROZPOCZYNAM WCZYTYWANIE MODELU WHISPER")
        print("=" * 80)
        print(f"[Preload] üìã Model: {model_name}")
        print(f"[Preload] üîß Device: {device}")
        print(f"[Preload] üîß Compute Type: {compute_type}")
        print(f"[Preload] ‚è≥ To mo≈ºe zajƒÖƒá 10-60 sekund...")
        print()
        
        try:
            import time
            import faster_whisper
            
            # Sprawd≈∫ u≈ºycie GPU przed wczytaniem
            if device.lower() == "cuda":
                try:
                    import torch
                    if torch.cuda.is_available():
                        mem_before = torch.cuda.memory_allocated(0) / 1024**2  # MB
                        print(f"[Preload] üìä GPU Memory przed wczytaniem: {mem_before:.1f} MB")
                        print(f"[Preload] üí° Sprawd≈∫ nvidia-smi - powinno pokazaƒá u≈ºycie GPU podczas wczytywania")
                except Exception as e:
                    print(f"[Preload] ‚ö†Ô∏è Nie mo≈ºna sprawdziƒá GPU memory: {e}")
            
            # Progress bar w logach
            start_time = time.time()
            steps = [
                "Inicjalizacja faster-whisper...",
                "Pobieranie modelu (je≈õli potrzebne)...",
                "Wczytywanie wag do pamiƒôci...",
                "Inicjalizacja CUDA/CPU...",
                "Gotowe!"
            ]
            
            for i, step in enumerate(steps):
                progress = (i + 1) / len(steps) * 100
                bar_length = 40
                filled = int(bar_length * (i + 1) / len(steps))
                bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)
                print(f"[Preload] [{bar}] {progress:.0f}% - {step}")
                time.sleep(0.3)  # Ma≈Çe op√≥≈∫nienie dla efektu
            
            # Rzeczywiste wczytywanie modelu
            print(f"[Preload] üîÑ Wczytywanie modelu {model_name}...")
            load_start = time.time()
            
            model = faster_whisper.WhisperModel(
                model_size_or_path=model_name,
                device=device,
                compute_type=compute_type
            )
            
            load_time = time.time() - load_start
            print(f"[Preload] ‚úÖ Model wczytany w {load_time:.1f} sekund!")
            
            # Sprawd≈∫ u≈ºycie GPU po wczytaniu
            if device.lower() == "cuda":
                try:
                    import torch
                    if torch.cuda.is_available():
                        mem_after = torch.cuda.memory_allocated(0) / 1024**2  # MB
                        mem_used = mem_after - mem_before
                        print(f"[Preload] üìä GPU Memory po wczytaniu: {mem_after:.1f} MB")
                        print(f"[Preload] üìä GPU Memory u≈ºyte przez model: {mem_used:.1f} MB")
                        print(f"[Preload] üí° Sprawd≈∫ nvidia-smi - powinno pokazaƒá u≈ºycie pamiƒôci GPU")
                except Exception as e:
                    print(f"[Preload] ‚ö†Ô∏è Nie mo≈ºna sprawdziƒá GPU memory: {e}")
            
            # Test transkrypcji (kr√≥tki test)
            print(f"[Preload] üß™ Testowanie modelu (kr√≥tki test)...")
            test_start = time.time()
            try:
                # Tworzymy kr√≥tki test audio (1 sekunda ciszy)
                import numpy as np
                test_audio = np.zeros(16000, dtype=np.float32)  # 1 sekunda @ 16kHz
                segments, info = model.transcribe(test_audio, language="pl", vad_filter=False)
                # Pobierz pierwszy segment (mo≈ºe byƒá pusty, to OK)
                list(segments)  # Wymuszenie przetworzenia
                test_time = time.time() - test_start
                print(f"[Preload] ‚úÖ Test zako≈Ñczony w {test_time:.1f} sekund")
            except Exception as e:
                print(f"[Preload] ‚ö†Ô∏è Test nie powi√≥d≈Ç siƒô (ale model jest wczytany): {e}")
            
            # Zwolnij model z pamiƒôci (zostanie wczytany ponownie w diarize.py, ale szybciej)
            del model
            if device.lower() == "cuda":
                try:
                    import torch
                    torch.cuda.empty_cache()
                except:
                    pass
            
            total_time = time.time() - start_time
            print()
            print("=" * 80)
            print(f"[Preload] ‚úÖ MODEL WCZYTANY POMY≈öLNIE!")
            print(f"[Preload] ‚è±Ô∏è  Ca≈Çkowity czas: {total_time:.1f} sekund")
            print("=" * 80)
            print()
            
            MODEL_PRELOADED = True
            return True
            
        except Exception as e:
            print()
            print("=" * 80)
            print(f"[Preload] ‚ùå B≈ÅƒÑD WCZYTYWANIA MODELU!")
            print("=" * 80)
            print(f"B≈ÇƒÖd: {e}")
            print()
            print("üí° Mo≈ºliwe przyczyny:")
            print("   - Brak po≈ÇƒÖczenia internetowego (pierwsze wczytywanie)")
            print("   - Za ma≈Ço pamiƒôci GPU/CPU")
            print("   - Nieprawid≈Çowa konfiguracja CUDA")
            print()
            print("‚ö†Ô∏è  Aplikacja bƒôdzie pr√≥bowaƒá wczytaƒá model przy pierwszym u≈ºyciu")
            print("=" * 80)
            print()
            return False

# ==================== DEEPGRAM DIARIZATION ====================
def run_deepgram_diarization_on_chunk(wav_bytes: bytes, lang: str) -> List[Segment]:
    """
    U≈ºywa Deepgram API do transkrypcji i diarization.
    Zwraca listƒô Segment√≥w w czasie LOKALNYM chunku (0..N sek).
    """
    print(f"[DEEPGRAM] üöÄ Rozpoczynam transkrypcjƒô i diarization przez Deepgram API...")
    
    try:
        import time
        start_time = time.time()
        
        # Przygotuj URL z parametrami
        url = "https://api.deepgram.com/v1/listen"
        params = {
            "diarize": "true",
            "punctuate": "true",
            "utterances": "true",
            "smart_format": "true",  # Lepsze formatowanie mo≈ºe pom√≥c w diarization
            "model": "nova-2",  # Najnowszy model - lepsze diarization
            "detect_language": "false",  # Wy≈ÇƒÖcz auto-detekcjƒô, u≈ºywamy wymuszonego jƒôzyka
        }
        
        # Dodaj jƒôzyk je≈õli podano
        if lang and lang.lower() != "auto":
            # Deepgram u≈ºywa kod√≥w ISO 639-1 (pl, en, de, fr, etc.)
            # Dla polskiego u≈ºywamy "pl"
            lang_map = {
                "pl": "pl",
                "polish": "pl",
                "en": "en",
                "english": "en",
                "de": "de",
                "german": "de",
                "fr": "fr",
                "french": "fr",
            }
            lang_code = lang_map.get(lang.lower(), lang.lower())
            params["language"] = lang_code
            print(f"[DEEPGRAM] üåê Ustawiono jƒôzyk: {lang_code} (z {lang})")
        
        # Przygotuj nag≈Ç√≥wki
        headers = {
            "Authorization": f"Token {DEEPGRAM_API_KEY}",
            "Content-Type": "audio/wav",
        }
        
        print(f"[DEEPGRAM] üì§ Wysy≈Çam {len(wav_bytes)} bajt√≥w audio do Deepgram API...")
        
        # Wy≈õlij request
        response = requests.post(
            url,
            params=params,
            headers=headers,
            data=wav_bytes,
            timeout=120  # 2 minuty timeout
        )
        
        response.raise_for_status()
        
        elapsed = time.time() - start_time
        print(f"[DEEPGRAM] ‚úÖ Odpowied≈∫ otrzymana w {elapsed:.1f}s")
        
        # Parsuj odpowied≈∫ JSON
        result = response.json()
        
        # Debug: wy≈õwietl strukturƒô odpowiedzi
        print(f"[DEEPGRAM] üîç Debug - struktura odpowiedzi:")
        if "results" in result:
            if "utterances" in result["results"]:
                print(f"[DEEPGRAM]   - utterances: {len(result['results']['utterances'])}")
            if "channels" in result["results"]:
                print(f"[DEEPGRAM]   - channels: {len(result['results']['channels'])}")
                for i, channel in enumerate(result["results"]["channels"]):
                    if "alternatives" in channel:
                        for j, alt in enumerate(channel["alternatives"]):
                            if "words" in alt:
                                print(f"[DEEPGRAM]   - channel[{i}].alternatives[{j}].words: {len(alt['words'])}")
                                # Sprawd≈∫ unikalnych m√≥wc√≥w w words (WSZYSTKIE s≈Çowa, nie tylko pierwsze 10)
                                speakers = set()
                                for word in alt["words"]:
                                    if "speaker" in word:
                                        speakers.add(word["speaker"])
                                print(f"[DEEPGRAM]   - Unikalni m√≥wcy w words (wszystkie {len(alt['words'])} s≈Ç√≥w): {speakers}")
        
        segments = []
        
        # Sprawd≈∫ najpierw words - mogƒÖ mieƒá lepsze informacje o m√≥wcach
        words_with_speakers = []
        if "results" in result and "channels" in result["results"]:
            for channel in result["results"]["channels"]:
                if "alternatives" in channel:
                    for alt in channel["alternatives"]:
                        if "words" in alt:
                            for word in alt["words"]:
                                if "speaker" in word:
                                    words_with_speakers.append(word)
        
        # Sprawd≈∫ unikalnych m√≥wc√≥w w words
        speakers_in_words = set()
        for word in words_with_speakers:
            speakers_in_words.add(word.get("speaker", 0))
        print(f"[DEEPGRAM] üîç Unikalni m√≥wcy w words: {speakers_in_words} (≈ÇƒÖcznie {len(words_with_speakers)} s≈Ç√≥w z informacjƒÖ o m√≥wcy)")
        
        # Spr√≥buj u≈ºyƒá utterances (je≈õli dostƒôpne) - sƒÖ ju≈º pogrupowane
        if "results" in result and "utterances" in result["results"]:
            utterances = result["results"]["utterances"]
            print(f"[DEEPGRAM] üìù Otrzymano {len(utterances)} utterances z diarization")
            
            # Sprawd≈∫ unikalnych m√≥wc√≥w w utterances
            speakers_in_utterances = set()
            for utterance in utterances:
                speaker = utterance.get("speaker", 0)
                speakers_in_utterances.add(speaker)
            print(f"[DEEPGRAM] üîç Unikalni m√≥wcy w utterances: {speakers_in_utterances}")
            
            # Je≈õli words majƒÖ wiƒôcej m√≥wc√≥w ni≈º utterances, u≈ºyj words zamiast utterances
            use_words_instead = False
            if len(speakers_in_words) > len(speakers_in_utterances) and len(speakers_in_words) > 1:
                print(f"[DEEPGRAM] ‚ö†Ô∏è Words majƒÖ wiƒôcej m√≥wc√≥w ({len(speakers_in_words)}) ni≈º utterances ({len(speakers_in_utterances)}) - u≈ºywam words!")
                use_words_instead = True
            elif len(speakers_in_utterances) == 1 and len(speakers_in_words) > 1:
                print(f"[DEEPGRAM] ‚ö†Ô∏è Utterances majƒÖ tylko 1 m√≥wcƒô, ale words majƒÖ {len(speakers_in_words)} - u≈ºywam words!")
                use_words_instead = True
            
            if not use_words_instead:
                # U≈ºyj utterances
                for utterance in utterances:
                    start = utterance.get("start", 0.0)
                    end = utterance.get("end", 0.0)
                    transcript = utterance.get("transcript", "").strip()
                    speaker = utterance.get("speaker", 0)
                    
                    if transcript:  # Tylko je≈õli jest tekst
                        speaker_str = f"Speaker {speaker}"
                        segments.append(Segment(
                            start=start,
                            end=end,
                            text=transcript,
                            speaker=speaker_str
                        ))
                
                # Je≈õli mamy segmenty z utterances, zwr√≥ƒá je
                if segments:
                    print(f"[DEEPGRAM] ‚úÖ Utworzono {len(segments)} segment√≥w z utterances")
                    return segments
        
        # Fallback: u≈ºyj words i grupuj rƒôcznie (lub je≈õli words majƒÖ wiƒôcej m√≥wc√≥w)
        words = []
        if "results" in result and "channels" in result["results"]:
            for channel in result["results"]["channels"]:
                if "alternatives" in channel:
                    for alt in channel["alternatives"]:
                        if "words" in alt:
                            words.extend(alt["words"])
        
        if not words:
            print("[DEEPGRAM] ‚ö†Ô∏è Brak s≈Ç√≥w w odpowiedzi!")
            return []
        
        print(f"[DEEPGRAM] üìù Otrzymano {len(words)} s≈Ç√≥w z diarization (grupowanie rƒôczne)")
        
        # Grupuj s≈Çowa w segmenty wed≈Çug m√≥wcy
        current_segment = None
        
        for word_data in words:
            word_text = word_data.get("word", "")
            start = word_data.get("start", 0.0)
            end = word_data.get("end", 0.0)
            speaker = word_data.get("speaker", 0)
            
            # Konwertuj speaker na string (Deepgram u≈ºywa liczb: 0, 1, 2, ...)
            speaker_str = f"Speaker {speaker}"
            
            # Je≈õli to pierwsze s≈Çowo lub m√≥wca siƒô zmieni≈Ç, rozpocznij nowy segment
            if current_segment is None or current_segment.speaker != speaker_str:
                # Zapisz poprzedni segment je≈õli istnieje
                if current_segment is not None:
                    segments.append(current_segment)
                
                # Rozpocznij nowy segment
                current_segment = Segment(
                    start=start,
                    end=end,
                    text=word_text,
                    speaker=speaker_str
                )
            else:
                # Dodaj s≈Çowo do obecnego segmentu
                current_segment.text += " " + word_text
                current_segment.end = end  # Aktualizuj koniec segmentu
        
        # Dodaj ostatni segment
        if current_segment is not None:
            segments.append(current_segment)
        
        print(f"[DEEPGRAM] ‚úÖ Utworzono {len(segments)} segment√≥w z diarization")
        return segments
        
    except requests.exceptions.RequestException as e:
        print(f"[DEEPGRAM] ‚ùå B≈ÇƒÖd request do Deepgram API: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                print(f"[DEEPGRAM] üìÑ Szczeg√≥≈Çy b≈Çƒôdu: {error_detail}")
            except:
                print(f"[DEEPGRAM] üìÑ Response: {e.response.text[:200]}")
        return []
    except Exception as e:
        print(f"[DEEPGRAM] ‚ùå Nieoczekiwany b≈ÇƒÖd: {e}")
        import traceback
        traceback.print_exc()
        return []

# ==================== WHISPER DIARIZATION (STARE) ====================
def get_gpu_compute_type(device: str) -> str:
    """
    Wykrywa architekturƒô GPU i zwraca odpowiedni compute_type.
    Starsze GPU (Pascal i starsze) nie wspierajƒÖ efektywnego float16.
    """
    if device.lower() != "cuda":
        return "float32"  # CPU zawsze u≈ºywa float32
    
    try:
        import torch
        if not torch.cuda.is_available():
            return "float32"
        
        # Pobierz compute capability GPU
        device_name = torch.cuda.get_device_name(0)
        compute_capability = torch.cuda.get_device_capability(0)
        major, minor = compute_capability
        
        print(f"[DIARIZE] üîç GPU: {device_name}, Compute Capability: {major}.{minor}")
        
        # Architektury GPU i ich compute capability:
        # Pascal (GTX 10xx): 6.0, 6.1, 6.2 - NIE wspiera efektywnego float16
        # Volta (V100): 7.0 - wspiera float16
        # Turing (RTX 20xx): 7.5 - wspiera float16
        # Ampere (RTX 30xx): 8.0, 8.6 - wspiera float16
        # Ada Lovelace (RTX 40xx): 8.9 - wspiera float16
        # Hopper (H100): 9.0 - wspiera float16
        
        if major < 7:
            print(f"[DIARIZE] ‚ö†Ô∏è GPU {device_name} (compute {major}.{minor}) nie wspiera efektywnego float16")
            print(f"[DIARIZE] üí° U≈ºywam float32 zamiast float16")
            return "float32"
        else:
            print(f"[DIARIZE] ‚úÖ GPU {device_name} (compute {major}.{minor}) wspiera float16")
            return "float16"
    except Exception as e:
        print(f"[DIARIZE] ‚ö†Ô∏è Nie mo≈ºna wykryƒá architektury GPU: {e}")
        print(f"[DIARIZE] üí° U≈ºywam bezpiecznego float32")
        return "float32"

def run_whisper_diarization_on_chunk(wav_bytes: bytes, lang: str, device: str) -> List[Segment]:
    """
    Uruchamia 'python diarize.py -a file.wav ...' i parsuje .srt.
    """
    global CUDA_FAILED
    
    # Je≈õli CUDA ju≈º nie dzia≈Ça≈Ço, od razu u≈ºyj CPU
    if CUDA_FAILED and device.lower() == "cuda":
        print("[DIARIZE] ‚ö†Ô∏è CUDA ju≈º wcze≈õniej nie dzia≈Ça≈Ço - u≈ºywam CPU")
        device = "cpu"
    
    # Wykryj odpowiedni compute_type dla GPU
    compute_type = get_gpu_compute_type(device)
    tmp_dir = tempfile.mkdtemp(prefix="wd_chunk_")
    wav_path = os.path.join(tmp_dir, "chunk.wav")
    with open(wav_path, "wb") as f:
        f.write(wav_bytes)

    diarize_dir = WHISPER_DIARIZATION_DIR or "./whisper-diarization"
    diarize_script = os.path.join(diarize_dir, "diarize.py")
    
    # Sprawd≈∫ czy katalog i skrypt istniejƒÖ
    if not os.path.isdir(diarize_dir):
        print(f"[DIARIZE] ERROR: Katalog nie istnieje: {diarize_dir}")
        shutil.rmtree(tmp_dir, ignore_errors=True)
        return []
    
    if not os.path.isfile(diarize_script):
        print(f"[DIARIZE] ERROR: Skrypt nie istnieje: {diarize_script}")
        shutil.rmtree(tmp_dir, ignore_errors=True)
        return []
    
    # U≈ºyj absolutnej ≈õcie≈ºki dla cwd (Windows wymaga tego)
    diarize_dir_abs = os.path.abspath(diarize_dir)
    
    # U≈ºyj tego samego interpretera Python co aplikacja
    python_executable = sys.executable
    
    # Dodaj katalog whisper-diarization do PYTHONPATH
    env = os.environ.copy()
    pythonpath = env.get("PYTHONPATH", "")
    
    parent_dir = os.path.dirname(diarize_dir_abs)
    ctc_aligner_dir = os.path.join(parent_dir, "ctc-forced-aligner")
    ctc_aligner_dir_alt = r"C:\Users\MSI\PycharmProjects\ctc-forced-aligner"
    
    separator = ";" if os.name == 'nt' else ":"
    paths_to_add = [diarize_dir_abs, parent_dir]
    
    # Sprawd≈∫ czy ctc-forced-aligner istnieje
    ctc_found_dir = None
    if os.path.isdir(ctc_aligner_dir):
        ctc_found_dir = ctc_aligner_dir
        paths_to_add.append(ctc_aligner_dir)
    elif os.path.isdir(ctc_aligner_dir_alt):
        ctc_found_dir = ctc_aligner_dir_alt
        paths_to_add.append(ctc_aligner_dir_alt)
    
    if ctc_found_dir:
        ctc_module_dir = os.path.join(ctc_found_dir, "ctc_forced_aligner")
        if os.path.isdir(ctc_module_dir):
            paths_to_add.append(ctc_module_dir)
    
    new_paths = separator.join(paths_to_add)
    if pythonpath:
        pythonpath = f"{new_paths}{separator}{pythonpath}"
    else:
        pythonpath = new_paths
    
    env["PYTHONPATH"] = pythonpath
    
    cmd = [
        python_executable, "-u", diarize_script,
        "-a", wav_path,
        "--no-stem",
        "--whisper-model", WHISPER_MODEL,
        "--device", device
    ]
    if lang and lang.lower() != "auto":
        cmd += ["--language", lang]
    
    # Ustaw compute_type dla starszych GPU (Pascal i starsze nie wspierajƒÖ float16)
    # Pr√≥bujemy dwa sposoby:
    # 1. Parametr --compute-type (je≈õli diarize.py go wspiera)
    # 2. Zmienna ≈õrodowiskowa (jako fallback)
    if compute_type == "float32" and device.lower() == "cuda":
        # Spos√≥b 1: Parametr --compute-type
        cmd += ["--compute-type", "float32"]
        # Spos√≥b 2: Zmienna ≈õrodowiskowa (dla faster-whisper)
        env["WHISPER_COMPUTE_TYPE"] = "float32"
        print(f"[DIARIZE] üîß Ustawiam compute_type=float32 dla kompatybilno≈õci z GPU (GTX 1050 Ti)")

    print(f"[DIARIZE] üöÄ Uruchamiam: {' '.join(cmd)}")
    print(f"[DIARIZE] üìÅ Katalog roboczy: {diarize_dir_abs}")
    print(f"[DIARIZE] üîß Device: {device}, Compute Type: {compute_type}")
    print(f"[DIARIZE] ‚è≥ To mo≈ºe zajƒÖƒá 10-60 sekund (zale≈ºnie od {device})...")
    if device.lower() == "cuda":
        print(f"[DIARIZE] üí° Sprawd≈∫ nvidia-smi w osobnym oknie, aby zobaczyƒá u≈ºycie GPU")

    try:
        import time
        diarize_start = time.time()
        result = subprocess.run(
            cmd, 
            cwd=diarize_dir_abs, 
            env=env,
            check=True, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True, 
            timeout=600
        )
        diarize_elapsed = time.time() - diarize_start
        print(f"[DIARIZE] ‚úÖ Proces zako≈Ñczony w {diarize_elapsed:.1f}s")
        
        if result.stdout:
            stdout_lines = result.stdout.strip().split('\n')
            if len(stdout_lines) > 0:
                print(f"[DIARIZE] üìÑ Stdout ({len(stdout_lines)} linii):")
                for line in stdout_lines[-10:]:
                    if line.strip():
                        print(f"[DIARIZE]    {line}")
        
        if result.stderr:
            stderr_lines = result.stderr.strip().split('\n')
            if len(stderr_lines) > 0:
                print(f"[DIARIZE] ‚ö†Ô∏è Stderr ({len(stderr_lines)} linii):")
                for line in stderr_lines[-10:]:
                    if line.strip():
                        print(f"[DIARIZE]    {line}")
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr if e.stderr else (e.stdout if e.stdout else 'Brak szczeg√≥≈Ç√≥w')
        print("=" * 80)
        print("[DIARIZE] ‚ùå B≈ÅƒÑD - Pe≈Çny komunikat b≈Çƒôdu:")
        print("=" * 80)
        print(error_msg)
        print("=" * 80)
        
        # Sprawd≈∫ czy to b≈ÇƒÖd brakujƒÖcego modu≈Çu
        missing_module = "ModuleNotFoundError" in error_msg or "No module named" in error_msg
        
        if missing_module:
            # Sprawd≈∫ kt√≥ry modu≈Ç brakuje
            if "nemo" in error_msg.lower():
                print("\n" + "=" * 80)
                print("[DIARIZE] ‚ö†Ô∏è BRAKUJE MODU≈ÅU: nemo (NVIDIA NeMo Toolkit)")
                print("=" * 80)
                print("‚ùå Problem: whisper-diarization wymaga NVIDIA NeMo Toolkit dla diarization")
                print()
                print("üí° RozwiƒÖzanie - Zainstaluj nemo:")
                print("   1. Aktywuj ≈õrodowisko Python (venv_test):")
                print("      venv_test\\Scripts\\activate")
                print()
                print("   2. Zainstaluj nemo:")
                print("      pip install nemo-toolkit[all]")
                print()
                print("   LUB tylko podstawowe (je≈õli powy≈ºsze nie dzia≈Ça):")
                print("      pip install nemo-toolkit")
                print()
                print("   3. Je≈õli masz problemy z instalacjƒÖ, sprawd≈∫:")
                print("      https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/getting_started/installation.html")
                print()
                print("‚ö†Ô∏è  UWAGA: nemo-toolkit mo≈ºe byƒá du≈ºy (~1-2GB) i wymagaƒá du≈ºo zale≈ºno≈õci")
                print("=" * 80)
                shutil.rmtree(tmp_dir, ignore_errors=True)
                return []
            elif "faster_whisper" in error_msg.lower():
                print("\n[DIARIZE] ‚ö†Ô∏è Brakuje faster-whisper - zainstaluj: pip install faster-whisper")
            elif "pyannote" in error_msg.lower():
                print("\n[DIARIZE] ‚ö†Ô∏è Brakuje pyannote.audio - zainstaluj: pip install pyannote.audio")
            else:
                # WyciƒÖgnij nazwƒô modu≈Çu z b≈Çƒôdu
                import re
                match = re.search(r"No module named ['\"]([^'\"]+)['\"]", error_msg)
                if match:
                    module_name = match.group(1)
                    print(f"\n[DIARIZE] ‚ö†Ô∏è Brakuje modu≈Çu: {module_name}")
                    print(f"[DIARIZE] üí° Zainstaluj: pip install {module_name}")
        
        # Sprawd≈∫ czy to b≈ÇƒÖd "unrecognized arguments" (diarize.py nie wspiera --compute-type)
        unrecognized_arg = "unrecognized arguments" in error_msg.lower() and "--compute-type" in error_msg.lower()
        
        if unrecognized_arg:
            print("\n" + "=" * 80)
            print("[DIARIZE] ‚ö†Ô∏è diarize.py NIE WSPIERA parametru --compute-type!")
            print("=" * 80)
            print("‚ùå Problem: diarize.py nie akceptuje --compute-type z linii polece≈Ñ")
            print("üí° RozwiƒÖzanie: Musisz zmodyfikowaƒá diarize.py aby u≈ºywa≈Ç compute_type='float32'")
            print()
            print("üìù Instrukcja:")
            print("   1. Otw√≥rz plik: C:\\Users\\MSI\\PycharmProjects\\whisper-diarization\\diarize.py")
            print("   2. Znajd≈∫ liniƒô z: faster_whisper.WhisperModel(...)")
            print("   3. Dodaj parametr: compute_type='float32'")
            print("   4. Przyk≈Çad:")
            print("      whisper_model = faster_whisper.WhisperModel(")
            print("          model_size_or_path=args.whisper_model,")
            print("          device=args.device,")
            print("          compute_type='float32',  # <-- DODAJ TO dla GTX 1050 Ti")
            print("      )")
            print()
            print("üí° Alternatywnie, mo≈ºesz dodaƒá argument do parsera w diarize.py:")
            print("      parser.add_argument('--compute-type', default='float16', ...)")
            print("=" * 80)
            shutil.rmtree(tmp_dir, ignore_errors=True)
            return []
        
        # Sprawd≈∫ czy to b≈ÇƒÖd float16 (starsze GPU nie wspierajƒÖ)
        float16_error = "float16 compute type" in error_msg.lower() and "do not support" in error_msg.lower()
        
        if float16_error and device.lower() == "cuda":
            print("\n" + "=" * 80)
            print("[DIARIZE] ‚ö†Ô∏è B≈ÅƒÑD FLOAT16 WYKRYTY!")
            print("=" * 80)
            print("‚ùå Problem: GPU nie wspiera efektywnego float16 (GTX 1050 Ti to Pascal)")
            print("üí° RozwiƒÖzanie: Musisz zmodyfikowaƒá diarize.py aby u≈ºywa≈Ç compute_type='float32'")
            print()
            print("üìù Instrukcja:")
            print("   1. Otw√≥rz plik: C:\\Users\\MSI\\PycharmProjects\\whisper-diarization\\diarize.py")
            print("   2. Znajd≈∫ liniƒô z: faster_whisper.WhisperModel(...)")
            print("   3. Dodaj parametr: compute_type='float32'")
            print("   4. Przyk≈Çad:")
            print("      whisper_model = faster_whisper.WhisperModel(")
            print("          model_size_or_path=args.whisper_model,")
            print("          device=args.device,")
            print("          compute_type='float32',  # <-- DODAJ TO")
            print("      )")
            print("=" * 80)
            shutil.rmtree(tmp_dir, ignore_errors=True)
            return []
        
        # Sprawd≈∫ czy to b≈ÇƒÖd "out of memory" (GPU ma za ma≈Ço VRAM)
        out_of_memory = "out of memory" in error_msg.lower() or "cuda oom" in error_msg.lower()
        
        if out_of_memory and device.lower() == "cuda":
            print("\n" + "=" * 80)
            print("[DIARIZE] ‚ö†Ô∏è B≈ÅƒÑD: CUDA OUT OF MEMORY!")
            print("=" * 80)
            print(f"‚ùå Problem: Model '{WHISPER_MODEL}' jest za du≈ºy dla GTX 1050 Ti (4GB VRAM)")
            print()
            print("üí° RozwiƒÖzania (w kolejno≈õci zalecanej):")
            print("   1. ‚úÖ U≈ºyj mniejszego modelu Whisper (NAJLEPSZE dla GTX 1050 Ti):")
            print("      - Otw√≥rz plik .env.test")
            print("      - Zmie≈Ñ: WHISPER_MODEL=medium")
            print("      - Lub: WHISPER_MODEL=small (jeszcze mniejszy)")
            print("      - Zrestartuj aplikacjƒô")
            print()
            print("   2. üîÑ LUB u≈ºyj CPU (wolniejsze, ale dzia≈Ça z large-v3):")
            print("      - W pliku .env.test ustaw: DEVICE=cpu")
            print("      - Zrestartuj aplikacjƒô")
            print()
            print("   3. üîß LUB zmniejsz batch_size (mniej efektywne):")
            print("      - Dodaj do wywo≈Çania: --batch-size 4")
            print("      - Wymaga modyfikacji kodu aplikacji")
            print()
            print("üìä Por√≥wnanie modeli:")
            print("   - base: ~74MB, najszybszy, najgorsza jako≈õƒá")
            print("   - small: ~244MB, szybki, dobra jako≈õƒá")
            print("   - medium: ~769MB, ≈õrednia prƒôdko≈õƒá, bardzo dobra jako≈õƒá")
            print("   - large-v3: ~1550MB, wolny, najlepsza jako≈õƒá (ZA DU≈ªY dla 4GB VRAM)")
            print("=" * 80)
            shutil.rmtree(tmp_dir, ignore_errors=True)
            return []
        
        # Sprawd≈∫ czy to b≈ÇƒÖd CUDA
        cuda_errors = [
            "CUDA driver version is insufficient",
            "CUDA error",
            "CUDA runtime",
        ]
        is_cuda_error = any(err.lower() in error_msg.lower() for err in cuda_errors)
        
        if is_cuda_error and device.lower() == "cuda":
            CUDA_FAILED = True
            
            print("\n" + "=" * 80)
            print("[DIARIZE] ‚ö†Ô∏è B≈ÅƒÑD CUDA WYKRYTY!")
            print("=" * 80)
            if "driver version is insufficient" in error_msg.lower():
                print("‚ùå Problem: Sterownik CUDA jest za stary dla CUDA runtime!")
                print("üí° RozwiƒÖzania:")
                print("   1. Zaktualizuj sterowniki NVIDIA GPU:")
                print("      - Pobierz najnowsze sterowniki z: https://www.nvidia.com/Download/index.aspx")
                print("      - Lub u≈ºyj GeForce Experience do automatycznej aktualizacji")
                print("   2. Tymczasowo u≈ºyj CPU: ustaw DEVICE=cpu w .env.test")
                print("   3. LUB zainstaluj starszƒÖ wersjƒô PyTorch z CUDA (zgodnƒÖ z twoim sterownikiem)")
            else:
                print("üí° RozwiƒÖzania:")
                print("   1. Zaktualizuj sterowniki GPU (NVIDIA)")
                print("   2. Ustaw DEVICE=cpu w .env.test")
            print("=" * 80)
            print("[DIARIZE] üîÑ Pr√≥bujƒô ponownie z CPU...")
            print("[DIARIZE] ‚ö†Ô∏è UWAGA: Wszystkie kolejne chunki bƒôdƒÖ przetwarzane na CPU (bƒôdzie wolniej)")
            print("=" * 80)
            
            # Retry z CPU
            cmd_cpu = cmd.copy()
            try:
                device_idx = cmd_cpu.index("--device")
                if device_idx + 1 < len(cmd_cpu):
                    cmd_cpu[device_idx + 1] = "cpu"
            except ValueError:
                cmd_cpu.extend(["--device", "cpu"])
            
            try:
                result = subprocess.run(
                    cmd_cpu,
                    cwd=diarize_dir_abs,
                    env=env,
                    check=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=600
                )
                diarize_elapsed = time.time() - diarize_start
                print(f"[DIARIZE] ‚úÖ Proces zako≈Ñczony w {diarize_elapsed:.1f}s (z CPU)")
            except Exception as retry_error:
                print(f"[DIARIZE] ‚ùå Retry z CPU te≈º nie powi√≥d≈Ç siƒô: {retry_error}")
                shutil.rmtree(tmp_dir, ignore_errors=True)
                return []
        else:
            shutil.rmtree(tmp_dir, ignore_errors=True)
            return []
    except subprocess.TimeoutExpired:
        print("[DIARIZE] ‚ùå Przekroczono limit czasu (600s = 10 minut)")
        shutil.rmtree(tmp_dir, ignore_errors=True)
        return []
    except (OSError, NotADirectoryError) as e:
        print(f"[DIARIZE] ‚ùå B≈ÇƒÖd systemowy: {e}")
        shutil.rmtree(tmp_dir, ignore_errors=True)
        return []

    candidates = glob.glob(os.path.join(tmp_dir, "*.srt"))
    if not candidates:
        base = os.path.splitext(os.path.basename(wav_path))[0]
        candidates = glob.glob(os.path.join(diarize_dir, f"{base}*.srt"))
    if not candidates:
        print("[DIARIZE] ‚ö†Ô∏è Nie znaleziono pliku .srt po diarization!")
        shutil.rmtree(tmp_dir, ignore_errors=True)
        return []
    srt_path = max(candidates, key=os.path.getmtime)
    print(f"[DIARIZE] üìÑ Znaleziono plik SRT: {srt_path}")

    entries = parse_srt(srt_path)
    segs = [Segment(start=s, end=e, text=t, speaker=spk) for (s, e, spk, t) in entries if t]
    print(f"[DIARIZE] üìù Sparsowano {len(entries)} wpis√≥w, {len(segs)} segment√≥w z tekstem")
    shutil.rmtree(tmp_dir, ignore_errors=True)
    return segs

# ==================== WORKER ====================
def worker_loop():
    """Worker w tle - przetwarza chunki audio"""
    while True:
        try:
            wav_bytes, chunk_offset = audio_q.get(timeout=0.2)
        except queue.Empty:
            continue

        print(f"[Worker] üéØ OTRZYMANO CHUNK! Offset: {chunk_offset:.2f}s, rozmiar: {len(wav_bytes)} bajt√≥w")
        print("[Worker] Rozpoczynam diarization...")
        
        # Aktualizuj status - rozpoczƒôcie przetwarzania
        with _ui_data_lock:
            _ui_data["current_chunk"] = _ui_data.get("chunks_processed", 0) + 1
            _ui_data["total_chunks"] = _ui_data.get("chunks_created", 0)
            _ui_data["processing_status"] = f"üîÑ Przetwarzanie chunka {_ui_data['current_chunk']}/{_ui_data['total_chunks']}..."
            _ui_data["processing_progress"] = 0.1
        
        try:
            import time
            start_time = time.time()
            
            # WƒÖtek do symulacji postƒôpu podczas diarization
            progress_stop_event = threading.Event()
            def simulate_progress():
                progress = 0.3
                while not progress_stop_event.is_set() and progress < 0.7:
                    time.sleep(1.0)
                    if not progress_stop_event.is_set():
                        progress += 0.02
                        if progress > 0.7:
                            progress = 0.7
                        with _ui_data_lock:
                            if _ui_data.get("processing_progress", 0) < 0.7:
                                _ui_data["processing_progress"] = progress
            
            progress_thread = threading.Thread(target=simulate_progress, daemon=True)
            progress_thread.start()
            
            try:
                # Diarization przez Deepgram API lub whisper-diarization
                if USE_DEEPGRAM:
                    segs_local = run_deepgram_diarization_on_chunk(
                        wav_bytes=wav_bytes,
                        lang=FORCED_LANG
                    )
                else:
                    segs_local = run_whisper_diarization_on_chunk(
                        wav_bytes=wav_bytes,
                        lang=FORCED_LANG,
                        device=DEVICE
                    )
            finally:
                progress_stop_event.set()
                progress_thread.join(timeout=0.1)
            
            elapsed_time = time.time() - start_time
            print(f"[Worker] ‚úÖ Diarization zako≈Ñczone! Zwr√≥cono {len(segs_local)} segment√≥w (czas: {elapsed_time:.1f}s)")
            
            if len(segs_local) == 0:
                print("[Worker] ‚ö†Ô∏è Brak segment√≥w w wyniku diarization!")
                with _ui_data_lock:
                    _ui_data["processing_progress"] = 0.0
                continue
            
            # Aktualizuj progress - diarization zako≈Ñczone, parsowanie
            with _ui_data_lock:
                _ui_data["processing_progress"] = 0.7
            
            # Dodaj segmenty do transkrypcji (z filtrowaniem duplikat√≥w z overlap)
            last_segment_end = 0.0
            if len(conversation_state.transcript) > 0:
                last_segment_end = conversation_state.transcript[-1].end
            
            # Filtruj segmenty, kt√≥re sƒÖ w obszarze overlap (pierwsze 2 sekundy chunka)
            overlap_start = chunk_offset
            overlap_end = chunk_offset + CHUNK_OVERLAP_SECONDS
            
            for s in segs_local:
                segment_start_global = s.start + chunk_offset
                segment_end_global = s.end + chunk_offset
                
                # Pomi≈Ñ segmenty, kt√≥re sƒÖ w obszarze overlap (ju≈º by≈Çy w poprzednim chunku)
                if segment_start_global < overlap_end and segment_start_global >= overlap_start:
                    # Segment zaczyna siƒô w obszarze overlap - sprawd≈∫ czy nie jest duplikatem
                    if segment_start_global < last_segment_end:
                        # Ten segment ju≈º by≈Ç w poprzednim chunku - pomi≈Ñ
                        continue
                
                conversation_state.transcript.append(
                    Segment(
                        start=segment_start_global,
                        end=segment_end_global,
                        text=s.text,
                        speaker=s.speaker
                    )
                )
            
            # Aktualizuj progress - zako≈Ñczone
            with _ui_data_lock:
                _ui_data["processing_progress"] = 0.9
            
            # Zaktualizuj UI - transkrypcja
            lines = []
            for seg in conversation_state.transcript[-MAX_RENDER_LINES:]:
                speaker_label = seg.speaker or "?"
                lines.append(f"[{seg.start:6.1f}‚Äì{seg.end:6.1f}] {speaker_label}: {seg.text}")
            
            with _ui_data_lock:
                _ui_data["transcript_text"] = "\n".join(lines)
                _ui_data["chunks_processed"] += 1
                _ui_data["processing_progress"] = 1.0
                _ui_data["processing_status"] = f"‚úÖ Chunek {_ui_data['chunks_processed']}/{_ui_data.get('chunks_created', 0)} przetworzony"
                print(f"[Worker] üìù Zaktualizowano transkrypcjƒô: {len(lines)} linii, ≈ÇƒÖcznie {len(conversation_state.transcript)} segment√≥w")
            
            # Zaktualizuj UI - m√≥wcy
            speakers = set()
            for seg in conversation_state.transcript:
                if seg.speaker:
                    speakers.add(seg.speaker)
            
            speakers_display = []
            if speakers:
                speakers_display.append("**Rozpoznani m√≥wcy:**\n")
                for speaker in sorted(speakers):
                    count = sum(1 for seg in conversation_state.transcript if seg.speaker == speaker)
                    speakers_display.append(f"üé§ **{speaker}:** {count} segment√≥w")
            else:
                speakers_display.append("_Oczekiwanie na rozpoznanie m√≥wc√≥w..._")
            
            with _ui_data_lock:
                _ui_data["speakers_text"] = "\n".join(speakers_display)
            
            # Reset progress po kr√≥tkiej chwili
            time.sleep(0.5)
            with _ui_data_lock:
                if _ui_data.get("chunks_processed", 0) >= _ui_data.get("chunks_created", 0):
                    _ui_data["processing_status"] = "‚úÖ Wszystkie chunki przetworzone"
                    _ui_data["processing_progress"] = 1.0
                else:
                    _ui_data["processing_progress"] = 0.0
            
        except Exception as e:
            print(f"[Worker] ‚ùå B≈ÅƒÑD podczas przetwarzania chunka: {e}")
            import traceback
            traceback.print_exc()
            with _ui_data_lock:
                _ui_data["processing_status"] = f"‚ùå B≈ÇƒÖd przetwarzania chunka {_ui_data.get('current_chunk', 0)}"
                _ui_data["processing_progress"] = 0.0

# Start worker thread
worker_thread = threading.Thread(target=worker_loop, daemon=True)
worker_thread.start()
print("[Main] ‚úÖ Worker thread uruchomiony")

# ==================== REAL-TIME AUDIO STREAMING ====================
_recording_active = False
_audio_buffer_lock = threading.Lock()
_audio_buffer = np.array([], dtype=np.float32)
_audio_stream = None
_recording_start_time = None
_chunk_counter = 0

def audio_callback(indata, frames, time_info, status):
    """Callback wywo≈Çywany przez sounddevice podczas nagrywania"""
    global _audio_buffer, _recording_active
    
    if not _recording_active:
        return
    
    if status:
        print(f"[Audio] ‚ö†Ô∏è Status: {status}")
    
    audio_chunk = indata[:, 0] if indata.ndim > 1 else indata
    audio_chunk = audio_chunk.astype(np.float32)
    
    with _audio_buffer_lock:
        _audio_buffer = np.concatenate([_audio_buffer, audio_chunk])
        
        chunk_samples = int(CHUNK_SECONDS * TARGET_SR)
        overlap_samples = int(CHUNK_OVERLAP_SECONDS * TARGET_SR)
        
        # Tworzymy chunki z overlap - ka≈ºdy chunk zaczyna siƒô 2 sekundy wcze≈õniej ni≈º poprzedni
        while len(_audio_buffer) >= chunk_samples:
            # We≈∫ chunk (10 sekund)
            chunk = _audio_buffer[:chunk_samples]
            
            # Zostaw overlap w buforze (2 sekundy) dla nastƒôpnego chunka
            # Usu≈Ñ tylko (chunk_samples - overlap_samples) z poczƒÖtku
            samples_to_remove = chunk_samples - overlap_samples
            _audio_buffer = _audio_buffer[samples_to_remove:]
            
            wav_bytes = to_wav_bytes(chunk, TARGET_SR)
            global _chunk_counter
            # Offset jest liczony bez overlap (ka≈ºdy chunk zaczyna siƒô 8 sekund po poprzednim zamiast 10)
            offset = _chunk_counter * (CHUNK_SECONDS - CHUNK_OVERLAP_SECONDS)
            _chunk_counter += 1
            audio_q.put((wav_bytes, offset))
            
            print(f"[Stream] ‚è∞ CHUNK GOTOWY! {CHUNK_SECONDS}s (z overlap {CHUNK_OVERLAP_SECONDS}s), offset: {offset:.1f}s, bufor: {len(_audio_buffer)/TARGET_SR:.1f}s")
            with _ui_data_lock:
                _ui_data["chunks_created"] += 1

def start_recording():
    """Rozpoczyna nagrywanie z mikrofonu"""
    global _recording_active, _audio_stream, _recording_start_time, _chunk_counter, _audio_buffer
    
    if not SOUNDDEVICE_AVAILABLE:
        return "‚ùå sounddevice nie jest zainstalowany! Zainstaluj: pip install sounddevice", "", ""
    
    if _recording_active:
        return "‚ö†Ô∏è Nagrywanie ju≈º trwa!", "", ""
    
    try:
        _recording_active = True
        _recording_start_time = time.time()
        _chunk_counter = 0
        with _audio_buffer_lock:
            _audio_buffer = np.array([], dtype=np.float32)
        
        conversation_state.started_at = _recording_start_time
        conversation_state.transcript = []
        
        _audio_stream = sd.InputStream(
            samplerate=TARGET_SR,
            channels=1,
            dtype=np.float32,
            callback=audio_callback,
            blocksize=int(TARGET_SR * 0.1)
        )
        _audio_stream.start()
        
        print(f"[Stream] üéµ ROZPOCZƒòTO NAGRYWANIE! Sample rate: {TARGET_SR}Hz")
        
        with _ui_data_lock:
            _ui_data["chunks_created"] = 0
            _ui_data["chunks_processed"] = 0
            _ui_data["processing_status"] = "‚è∏Ô∏è Oczekiwanie na chunki..."
            _ui_data["processing_progress"] = 0.0
        
        transcript, speakers, status, proc_status, proc_progress = get_current_status()
        return "‚úÖ Nagrywanie rozpoczƒôte! M√≥w do mikrofonu...", transcript, speakers
    except Exception as e:
        _recording_active = False
        error_msg = f"‚ùå B≈ÇƒÖd rozpoczƒôcia nagrywania: {e}"
        print(f"[Stream] {error_msg}")
        return error_msg, "", ""

def stop_recording():
    """Zatrzymuje nagrywanie"""
    global _recording_active, _audio_stream
    
    if not _recording_active:
        transcript, speakers, status, proc_status, proc_progress = get_current_status()
        return "‚ö†Ô∏è Nagrywanie nie jest aktywne!", transcript, speakers, status, proc_status, proc_progress
    
    try:
        _recording_active = False
        
        if _audio_stream is not None:
            _audio_stream.stop()
            _audio_stream.close()
            _audio_stream = None
        
        with _audio_buffer_lock:
            if len(_audio_buffer) > 0:
                remaining_seconds = len(_audio_buffer) / TARGET_SR
                print(f"[Stream] ‚èπÔ∏è Nagranie zako≈Ñczone. Reszta w buforze: {remaining_seconds:.2f}s")
        
        print("[Stream] ‚èπÔ∏è Nagrywanie zatrzymane")
        
        transcript, speakers, status, proc_status, proc_progress = get_current_status()
        return "‚èπÔ∏è Nagrywanie zatrzymane", transcript, speakers, status, proc_status, proc_progress
    except Exception as e:
        error_msg = f"‚ùå B≈ÇƒÖd zatrzymania nagrywania: {e}"
        print(f"[Stream] {error_msg}")
        return error_msg, "", "", "", "‚ùå B≈ÇƒÖd", 0

def reset_audio_buffer():
    """Reset stanu nagrywania"""
    global _recording_active, _audio_stream, _audio_buffer, _chunk_counter
    
    if _recording_active:
        stop_recording()
    
    conversation_state.transcript = []
    conversation_state.started_at = time.time()
    _chunk_counter = 0
    
    with _audio_buffer_lock:
        _audio_buffer = np.array([], dtype=np.float32)
    
    with _ui_data_lock:
        _ui_data["transcript_text"] = ""
        _ui_data["speakers_text"] = "_Oczekiwanie na transkrypcjƒô..._"
        _ui_data["chunks_created"] = 0
        _ui_data["chunks_processed"] = 0
        _ui_data["processing_status"] = "‚è∏Ô∏è Oczekiwanie na chunki..."
        _ui_data["processing_progress"] = 0.0
    
    print("[Reset] üîÑ Stan zresetowany")

def get_current_status():
    """Zwraca aktualny status dla auto-refresh"""
    with _ui_data_lock:
        transcript = _ui_data.get("transcript_text", "")
        speakers = _ui_data.get("speakers_text", "_Oczekiwanie na transkrypcjƒô..._")
        chunks_created = _ui_data.get("chunks_created", 0)
        chunks_processed = _ui_data.get("chunks_processed", 0)
        processing_status = _ui_data.get("processing_status", "‚è∏Ô∏è Oczekiwanie na chunki...")
        processing_progress = _ui_data.get("processing_progress", 0.0)
    
    status = f"""
**üìä Statystyki:**
- Chunki utworzone: {chunks_created}
- Chunki przetworzone: {chunks_processed}
- Segmenty: {len(conversation_state.transcript)}
- Kolejka: {audio_q.qsize()}
"""
    progress_percent = int(processing_progress * 100)
    return transcript, speakers, status, processing_status, progress_percent

# Tworzenie UI
with gr.Blocks(title="Live Transcription + Diarization") as demo:
    gr.Markdown("# üéôÔ∏è Transkrypcja na ≈ºywo + Diarization")
    if USE_DEEPGRAM:
        gr.Markdown("**Streaming w czasie rzeczywistym:** Kliknij 'Start nagrywania' i m√≥w do mikrofonu. Co 10 sekund audio jest automatycznie przetwarzane przez Deepgram API w celu transkrypcji i rozpoznania m√≥wc√≥w.")
    else:
        gr.Markdown("**Streaming w czasie rzeczywistym:** Kliknij 'Start nagrywania' i m√≥w do mikrofonu. Co 10 sekund audio jest automatycznie przetwarzane przez whisper-diarization (CUDA) w celu rozpoznania m√≥wc√≥w.")
    
    with gr.Row():
        with gr.Column(scale=1):
            transcript_output = gr.Textbox(
                label="üìù Transkrypcja",
                lines=20,
                max_lines=30,
                interactive=False,
                placeholder="_Oczekiwanie na audio..._"
            )
        with gr.Column(scale=1):
            speakers_output = gr.Markdown(
                label="üë• Rozpoznani m√≥wcy",
                value="_Oczekiwanie na transkrypcjƒô..._"
            )
            status_output = gr.Markdown(
                label="üìä Status",
                value="_Oczekiwanie..._"
            )
    
    # Progress bar dla przetwarzania chunk√≥w
    with gr.Row():
        processing_status_text = gr.Textbox(
            label="üîÑ Status przetwarzania",
            value="‚è∏Ô∏è Oczekiwanie na chunki...",
            interactive=False
        )
        processing_progress_bar = gr.Slider(
            label="Postƒôp",
            minimum=0,
            maximum=100,
            value=0,
            interactive=False,
            info="Postƒôp przetwarzania chunka (0-100%)"
        )
    
    # Przyciski kontroli nagrywania
    with gr.Row():
        start_btn = gr.Button("‚ñ∂Ô∏è Start nagrywania", variant="primary")
        stop_btn = gr.Button("‚èπÔ∏è Stop nagrywania", variant="stop")
        reset_btn = gr.Button("üîÑ Reset", variant="secondary")
        refresh_btn = gr.Button("üîÑ Od≈õwie≈º status", variant="secondary")
    
    # Status nagrywania
    recording_status = gr.Textbox(
        label="üìä Status nagrywania",
        value="‚è∏Ô∏è Nagrywanie nieaktywne",
        interactive=False
    )
    
    # Inicjalizacja przy starcie
    demo.load(
        fn=get_current_status,
        inputs=None,
        outputs=[transcript_output, speakers_output, status_output, processing_status_text, processing_progress_bar]
    )
    
    # Automatyczne od≈õwie≈ºanie - u≈ºyjmy prostego podej≈õcia z JavaScript
    demo.load(
        fn=None,
        js="""
        () => {
            setInterval(() => {
                const buttons = Array.from(document.querySelectorAll('button'));
                const refreshBtn = buttons.find(btn => btn.textContent && btn.textContent.includes('Od≈õwie≈º status'));
                if (refreshBtn && !refreshBtn.disabled) {
                    refreshBtn.click();
                }
            }, 2000);
            return [];
        }
        """
    )
    
    # Rƒôczne od≈õwie≈ºanie statusu
    refresh_btn.click(
        fn=get_current_status,
        inputs=None,
        outputs=[transcript_output, speakers_output, status_output, processing_status_text, processing_progress_bar]
    )
    
    # Kontrola nagrywania
    start_btn.click(
        fn=start_recording,
        inputs=None,
        outputs=[recording_status, transcript_output, speakers_output]
    )
    
    stop_btn.click(
        fn=stop_recording,
        inputs=None,
        outputs=[recording_status, transcript_output, speakers_output, status_output, processing_status_text, processing_progress_bar]
    )
    
    # Reset bufora
    reset_btn.click(
        fn=reset_audio_buffer,
        inputs=None,
        outputs=None
    )
    
    gr.Markdown("---")
    if USE_DEEPGRAM:
        gr.Markdown("**Silnik:** Deepgram API (Cloud) | **Diarization:** W≈ÇƒÖczone")
    else:
        gr.Markdown("**Silnik:** whisper-diarization (CUDA) | **Model:** Whisper Large V3")

if __name__ == "__main__":
    print("[Main] üöÄ Inicjalizacja aplikacji...")
    
    # Wstƒôpne wczytanie modelu przed uruchomieniem Gradio (tylko je≈õli nie u≈ºywamy Deepgram)
    if not USE_DEEPGRAM and DEVICE_CONFIG.lower() == "cuda":
        compute_type = get_gpu_compute_type(DEVICE_CONFIG)
        print(f"[Main] üìã Konfiguracja: Device={DEVICE_CONFIG}, ComputeType={compute_type}, Model={WHISPER_MODEL}")
        print(f"[Main] üí° Wczytujƒô model przed startem aplikacji...")
        print(f"[Main] üí° Sprawd≈∫ nvidia-smi w osobnym oknie, aby zobaczyƒá u≈ºycie GPU podczas wczytywania")
        print()
        
        # Wczytaj model w tle (w osobnym wƒÖtku, aby nie blokowaƒá)
        def preload_in_background():
            preload_whisper_model(DEVICE_CONFIG, compute_type, WHISPER_MODEL)
        
        preload_thread = threading.Thread(target=preload_in_background, daemon=True)
        preload_thread.start()
        print("[Main] üí° Wczytywanie modelu rozpoczƒôte w tle...")
        print("[Main] üí° Aplikacja uruchomi siƒô, ale model bƒôdzie wczytywany r√≥wnolegle")
        print()
    else:
        print(f"[Main] üìã Konfiguracja: Device={DEVICE_CONFIG}, Model={WHISPER_MODEL}")
        print(f"[Main] ‚ö†Ô∏è  U≈ºywam CPU - wczytywanie modelu przy pierwszym u≈ºyciu")
        print()
    
    print("[Main] üöÄ Uruchamiam Gradio...")
    demo.queue()
    demo.launch(share=False, server_name="127.0.0.1", server_port=7861)

